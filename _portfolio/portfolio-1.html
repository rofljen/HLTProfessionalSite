---
title: "Bridging the Digital Divide: Where Do We Stand?"
excerpt: During my internship with XRI Global, I applied skills from both the MSHLT program and my experience as a data analyst to contribute to the development of a custom text classification pipeline using tools like Python, spaCy, and PostgreSQL. This experience deepened my understanding of NLP and prepared me for future work in human language technology.
collection: portfolio
layout: default
sidebar: false
---

<div class="summary">Summary:</div>
<div class="summary-content">
  As an intern with XRI Global, I collaborated with staff and fellow University of Arizona HLT students on a project aimed at supporting low-resource languages by improving their digital presence.<br><br>
  Our team inventoried existing datasets and language models from platforms like Hugging Face, GitHub, and Mozilla Common Voice. For some languages where models were not available, our team trained models on these datasets.<br><br>
  My contributions included designing the underlying database schema, identifying data standards, streamlining data collection workflows, and cleaning and standardizing data.<br><br>
  The resulting data powers a user-friendly web interface that allows researchers and developers to easily locate and access resources — helping bridge the digital divide for underrepresented languages.
</div>

<div class="content-head">Content:</div>
<div class="content">
  My role in the project included cleaning and processing large volumes of multilingual text, training models, and building custom evaluation scripts. I utilized tools such as Python, spaCy, scikit-learn, and PostgreSQL.<br><br>

  I applied course knowledge from Human Language Technology classes like NLP, Corpus Linguistics, and Machine Learning. I also deepened my understanding of real-world data challenges, annotation inconsistencies, and pipeline deployment.<br><br>

  This internship sharpened my programming and problem-solving skills and gave me hands-on experience with collaborative version control, agile workflows, and working with clients to define project scope.
</div>

<details class="collapsible-section">
  <summary class="content-head">Requirements Gathering</summary>
  <div class="content">
    <h4>Name & Classification</h4>
    <p>
      Pulled each language’s name and its family / sub-family from Glottolog
      (e.g. “Akan → Niger–Congo → Atlantic–Congo”) to situate it in the genetic tree
      and enable branch-level filtering.
    </p>
    <p><strong>Schema:</strong> language_new.language_name, language_new.language_family_id,<br>
       language_new.language_subfamily_id, language_new.glottocode
    </p>

    <h4>Location</h4>
    <p>
      Retrieved centroid latitude/longitude from GeoNames and stored it as a
      POINT—so you can plot dialect coverage or target nearby speakers for ASR collection.
    </p>
    <p><strong>Schema:</strong> language_new.geo_center (PostGIS POINT)</p>

    <h4>Global Region</h4>
    <p>
      Mapped every lat/long to its continent or sub-continent (e.g. “West Africa”,
      “Southeast Asia”) for regional roll-ups in your dashboards.
    </p>
    <p><strong>Schema:</strong> language_new.region</p>

    <h4>Number of Speakers</h4>
    <p>
      Collected L1 and L2 speaker estimates (with year tags) to visualize each
      language’s scale and automatically highlight ultra-low-resource targets (< 10 K L1).
    </p>
    <p><strong>Schema:</strong> language_new.l1_speakers, language_new.l2_speakers,<br>
       language_new.speaker_year
    </p>

    <h4>Language Codes</h4>
    <p>
      Standardized on ISO 639-1 (2-letter), ISO 639-3 (3-letter) and the Glottocode
      so every dataset, model, or API call can unambiguously reference the right language.
    </p>
    <p><strong>Schema:</strong> language_new.iso_639_1, language_new.iso_639_3,<br>
       language_new.glottocode
    </p>
  </div>
</details>
<div class="code-link">
  <strong>Code:</strong> 
  <a href="https://github.com/yourusername/yourproject" target="_blank">View Project on GitHub</a>
</div>
