---
title: "Bridging the Digital Divide:<br>Where Do We Stand?"
excerpt: During my internship with XRI Global, I applied skills from both the MSHLT program and my experience as a data analyst to contribute to the development of a user-friendly resource for Language Technology models and data for primarily low-resource languages.
layout: default
author_profile: false
---

  <h2>Summary</h2>
<div class="card">
  <p>
    As an intern with XRI Global, I collaborated with staff and fellow University of Arizona HLT students on a project aimed at supporting low-resource languages by improving their digital presence.
  </p>
  <p>
    Our team inventoried existing datasets and language models from platforms like Hugging Face, GitHub, and Mozilla Common Voice. For some languages where models were not available, our team trained models on these datasets.
  </p>
  <p>
    My contributions included designing the underlying database schema, identifying data standards, streamlining data collection workflows, and cleaning and standardizing data.
  </p>
  <p>
    The resulting data powers a user-friendly web interface that allows researchers and developers to easily locate and access resources—helping bridge the digital divide for underrepresented languages.
  </p>
</div>

  <h2>Content</h2>
<div class="card">
  <p>
    My role in the project included cleaning and processing large volumes of multilingual text, training models, and building custom evaluation scripts. I utilized tools such as Python, spaCy, scikit-learn, and PostgreSQL.
  </p>
  <p>
    I applied course knowledge from Human Language Technology classes like NLP, Corpus Linguistics, and Machine Learning. I also deepened my understanding of real-world data challenges, annotation inconsistencies, and pipeline deployment.
  </p>
  <p>
    This internship sharpened my programming and problem-solving skills and gave me hands-on experience with collaborative version control, agile workflows, and working with clients to define project scope.
  </p>
</div>

<h2>Process</h2>
<details class="card collapsible-section">
  <summary><strong>Requirements Gathering</strong></summary>
  <div class="card-content">
    <h4>Name & Classification</h4>
    <p>
      Pulled each language’s name, family, and sub-family from Glottolog
      (e.g., “Akan → Niger–Congo → Atlantic–Congo”) to structure the hierarchy
      and enable filtering by genetic lineage.
    </p>
    <p><strong>Schema:</strong> language_new.language_name, language_new.language_family_id, language_new.language_subfamily_id, language_new.glottocode</p>

    <h4>Location</h4>
    <p>
      Retrieved each language’s geographic centroid using GeoNames and stored it
      as a PostGIS POINT, allowing spatial queries like mapping coverage areas.
    </p>
    <p><strong>Schema:</strong> language_new.geo_center (PostGIS geometry)</p>

    <h4>Global Region</h4>
    <p>
      Linked each language to a continent or subregion for aggregation and regional dashboards.
    </p>
    <p><strong>Schema:</strong> language_new.continent_or_region_id</p>

    <h4>Number of Speakers</h4>
    <p>
      Recorded estimated speaker populations to prioritize support for low-resource languages.
      (Speaker counts pulled from external sources tracked separately.)
    </p>
    <p><strong>Schema:</strong> language_new.num_speakers, pop_source</p>

    <h4>Language Codes</h4>
    <p>
      Standardized ISO 639-1, ISO 639-3, and Glottocode identifiers across the dataset
      to ensure consistent references for downstream NLP pipelines.
    </p>
    <p><strong>Schema:</strong> language_new.iso_639_1, language_new.iso_639_3, language_new.glottocode</p>

    <h4>Available Models & Datasets</h4>
    <p>
      Cataloged available ASR, TTS, and NMT models and datasets from platforms like Hugging Face and Common Voice.
      Where models were unavailable, tracked available source datasets.
    </p>
    <p><strong>Schema:</strong> nmt_datasets, nmt_pairs_source, asr_source, language_new.asr, language_new.tts, language_new.nmt</p>
  </div>
</details>

<div class="mermaid">
flowchart TD
  subgraph RG [Requirements Gathering]
    A1["Pull names & families\nfrom Glottolog"]
    A2["Retrieve centroids\nvia GeoNames"]
    A3["Collect speaker counts\n& sources"]
    A4["Standardize ISO-639 &\nGlottocode IDs"]
  end

  subgraph DS [Database Schema Design]
    B1[(language_new\nname, family_id,\nsubfamily_id, glottocode)]
    B2[(language_new\ngeo_center: POINT)]
    B3[(language_new\ncontinent_or_region_id)]
    B4[(language_new\nnum_speakers,\npop_source)]
    B5[(language_new\niso_639_1,\niso_639_3, glottocode)]
    B6[(model flags:\nASR, TTS, NMT)]
    B7[(nmt_datasets,\nnmt_pairs_source,\nasr_source)]
  end

  subgraph DC [Data Collection & Cleaning]
    C1["Ingest datasets\nfrom HF & Common Voice"]
    C2["Normalize table formats\n& handle nulls"]
    C3["Validate location &\nspeaker fields"]
  end

  subgraph MT [Model Training & Evaluation]
    D1["Train ASR models\n(Kaldi, Whisper…)"]
    D2["Train TTS models\n(Tacotron2, FastSpeech…)"]
    D3["Train NMT models\n(MarianNMT…)"]
    D4["Run evaluation scripts\n(WER, MOS, BLEU…)"]
  end

  subgraph UI [Web Interface]
    E1["Build interactive map\n& filters"]
    E2["Implement search by\nfamily/region"]
    E3["Link to HF/GitHub assets"]
  end

  RG --> DS --> DC --> MT --> UI
  A1 --> B1
  A2 --> B2
  A3 --> B4
  A4 --> B5
  B6 --> C1
  B7 --> C1
  C1 --> D1
  C1 --> D2
  C1 --> D3
  D4 --> E1
  D4 --> E2
  D4 --> E3

</div>


<div class="card code-link">
  <p><strong>Code:</strong> 
  <a href="https://github.com/yourusername/yourproject" target="_blank">View Project on GitHub</a></p>
</div>
